\section{The Problems of Sparse Coding}
\subsection{Sparse Coding}
There are many basic different aspects in sparse coding problems such as finding the coefficients, learning the basis vectors, matrix factorization etc.
At the very first, we briefly introduce some popular sparse methods for machine learning.
Recall that the target of sparse coding is to find as small number of basis vectors as possible to represent an input vector.
A common formulation for this problem is that:
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{a}} & \|\mathbf{a}\|_0\\
\mathrm{s.t.} & \|\mathbf{y}-\mathbf{X}\mathbf{a}\|_2 \leq \epsilon.
\end{array}
\label{eq-sparse}
\end{equation}
Here $\mathbf{X}=(\mathbf{x}_1,\,\mathbf{x}_2,\cdots ,\,\mathbf{x}_n)$ denotes the dictionary matrix collecting all basis vectors.
Unfortunately, $\|\mathbf{x}\|_0$ is not differentiable or even continuous.
Thus this optimization problem is a NP-hard problem and cannot be easily solved.
Generally, we are not able to obtain the optimal solution of Problem~\ref{eq-sparse}.
But many approximated solvers have been created to solve this problem.
Matching pursuit (MP), orthogonal matching pursuit (OMP) iteratively add the best basis vector to represent $\mathbf{x}$ which is an approximating solution of problem:
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{a}} & \|\mathbf{y}-\mathbf{X}\mathbf{a}\|_2^2 \\
\mathrm{s.t.} & \|\mathbf{a}\|_0 \leq s
\end{array}
\end{equation}
Here $s$ denotes the number of chosen basis vectors which stands for the sparsity of this problem.
Under certain assumptions, the solution of OMP algorithm converges to the real solution.

Another widely-used technique for solving sparse problem is to use $\ell_1$ norm to replace $\ell_0$ norm in Problem~\ref{eq-sparse}.
The least absolute shrinkage and selection operator (LASSO) approach uses the constraint that $\|\mathbf{a}\|_1$ is no greater than a given value $\epsilon$
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{a}} & \|\mathbf{y}-\mathbf{X}\mathbf{a}\|_2^2\\
\mathrm{s.t.} & \|\mathbf{a}\|_1 \leq \epsilon
\end{array}
\end{equation}
This problem is also equivalent to an unconstrained minimization with a specific $\beta$ of:
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{a}} & \|\mathbf{y}-\mathbf{X}\mathbf{a}\|_2^2 + \beta \|\mathbf{a}\|_1
\end{array}
\end{equation}
This problem is solved using general convex optimization methods like quadratic programming, as well as by specific approaches like the least angle regression algorithm.

\subsection{Dictionary Learning.}
As discussed above, we show some basic methods of finding the coefficients corresponding to a given input vector when the basis vectors are given.
Another big collection of problems discusses algorithms of learning the dictionary or the basis vectors.
One algorithm called K-SVD uses an iterative procedure by updating the coefficient matrix $\mathbf{A}$ and dictionary $\mathbf{X}$ separately
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{X},\mathbf{A}} & \|\mathbf{Y}-\mathbf{X}\mathbf{A}\| \\
\mathrm{s.t.} & \|\mathbf{a}_i\|_0 \leq s,\,\forall i\in\{1,\,2,\cdots ,\,m\}
\end{array}
\end{equation}

Here $\mathbf{Y}=(\mathbf{y}_1,\,\mathbf{y}_2,\cdots ,\,\mathbf{y}_m)$ contains all input vectors and $\mathbf{A} = (\mathbf{a}_1,\,\mathbf{a}_2,\cdots ,\,\mathbf{a}_m)$ is the corresponding coefficient matrix.
To solve this problem, we first initialize the dictionary using known overcomplete basis vector like wavelet, discrete cosine transformations etc or using random vectors from  input vectors.
Then

\subsection{Low rank representation and robust principle component analysis}
Sparse-based matrix factorization technique is another hot field in machine learning.
A typical technique is low rank representation which attemps to decompose any given matrix $M$ into a low rank matrix $L$ and a residual matrix $S$.
The residual matrix $S$ may have some specific properties like the input is corrupted by Gaussian noise or spase noise.
Under the assumption that $M$ is corrupted by Gaussian noise, a low rank problem is formulated as:
\begin{equation}
\label{eq-lowrank}
\begin{array}{cl}
\min_{L,S} & \|S\|_F\\
\mathrm{s.t.} & \mathrm{rank}(L)\leq r\\
&M=L+S
\end{array}
\end{equation}
here $r\ll \min(m,n)$ where $M$ is a $m\times n$ matrix.
The above problem is equivalent to principle component analysis (PCA) according to~\cite{jolliffe2005principal,wold1987principal}.
This problem is easily solved by first computing the singular value decomposition (SVD) of $M$ and then projecting the columns of $M$ onto the subspace spanned by the $r$ principle left singular vectors.
%Low rank representation attempts to approxmate
%\paragraph{Low rank representation}

However, PCA assumes that corruption is caused by Gaussian noise.
The result of PCA can be arbitraryly far from $M$ if only a few entry of $M$ is corrupted.
\cite{wright2009robust,candes2011robust} show that under some conditions that $S$ is rather sparse, one can exactly recover $M$ by solving
%If $S$ is assumed to be sparse, the problem becomes:
\begin{equation}
\label{eq-lowranksparse}
\begin{array}{cl}
\min_{L,S} & \|L\|_* + \lambda \|S\|_1 \\
\mathrm{s.t.} & M = L+S
\end{array}
\end{equation}

The formulation is obtained by relaxing following problem replacing the $\ell_0$ norm with the $\ell_1$ norm:
\begin{equation}
\label{eq-lowrankorigin}
\begin{array}{cl}
\min_{L,S} & \mathrm{rank}(L) + \lambda \|S\|_0 \\
\mathrm{s.t.} & M = L+S
\end{array}
\end{equation}

\cite{candes2009exact,wright2009robust} shows the uniqueness of the solution and~\cite{lin2010augmented} discusses efficient algorithm for solving low rank problem.
Generally, problem~\ref{eq-lowranksparse} can be treated as a general convex optimization problem and solved by any off-the-shelf interior point solver (like CVX~\cite{grant2008cvx}).

\paragraph{Numerical solution}
Here we introduce several popular ways to solve low-rank (robust PCA) problem in the form of~\eqref{eq-lowranksparse}.
The iterative thresholding approach introduced in~\cite{wright2009robust} solves a relaxed convex problem of \eqref{eq-lowranksparse}
\begin{equation}
\begin{array}{cl}
\min_{L,S} & \|L\|_*+\lambda\|S\|_1 +\frac{1}{2\tau} \|L\|_F^2 + \frac{1}{2\tau} \|S\|_F^2\\
\mathrm{s.t.} & M = L+S
\end{array}
\end{equation}
Another technique to solve this problem is the accelerated proximal gradient approach~\cite{beck2009fast}.
%It is a critical task to find a low rank representation or approximation for a general data matrix.

But low rank representation has been widely used in many applications like image compression, noise reduction, principle component analysis, regularization for ill-posed problems and so on.
%The basic problem of low rank is to approximate a given matrix $A$ using a matrix $B$ with low rank. 