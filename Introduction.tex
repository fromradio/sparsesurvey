\section{Introduction}
\label{sec:introduction}

Because of the fast development of Internet and other electronic equipments, the size of dataset is becoming  incredibly massive.
How to extract compact knowledge from such massive datasets is yet to be resolved.
At the same time, the dimension of data becomes much higher than before.
Thus how to extract low-dimensional structures from high-dimensional data is another serious problem in modern signal processing.

To solve these two challenging problems, sparsity-based approaches have been successfully introduced in many applications.
Sparse representation which models data vectors as sparse linear combinations of basis elements, is widely used in machine learning, signal processing, neuroscience and statistics.
Dictionary learning learns an overcomplete dictionary which owns the ability to represent given signals.
Low rank representation which decomposes a given matrix into a low rank matrix and residual with certain property.
So far sparse techniques have become state-of-art tools in many fields like machine learning, data mining, computer vision, pattern recognition etc.
%Sparse representation of signals has been drawing much attention of the researchers.

In geometric processing and computer graphics, people start to find out the advantages of sparse techniques.
Better results are obtained with sparse techniques.
At the same time, most formulations cannot be directly applied on geometric problems.
Thus many non-trivial problems must be solved while applying sparse techniques. 
We would like to show how sparse technique, a strong tool in machine learning is brought into a fresh filed, geometric processing.

In the rest of this paper, we first introduce traditional sparse models used in machine learning and computer vision.
Then we illustrate how people in geometric processing use sparse techniques in different applications.
