\section{The Problems of Sparse Coding}
\label{sec:SparseProblem}
There are many basic different aspects in sparse coding problems such as finding the coefficients, learning the basis vectors, matrix factorization etc.
At the very first, we briefly introduce some popular sparse methods for machine learning.
Recall that the target of sparse coding is to find as small number of basis vectors as possible to represent an input vector.
A common formulation for this problem is that:

\small{
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{a}} & \|\mathbf{a}\|_0\\
\mathrm{s.t.} & \|\mathbf{y}-\mathbf{X}\mathbf{a}\|_2 \leq \epsilon.
\end{array}
\label{eq-sparse}
\end{equation}
}

Here $\mathbf{X}=(\mathbf{x}_1,\,\mathbf{x}_2,\cdots ,\,\mathbf{x}_n)$ denotes the dictionary matrix collecting all basis vectors.
Unfortunately, $\|\mathbf{x}\|_0$ is not differentiable or even continuous.
Thus this optimization problem is a NP-hard problem and cannot be easily solved.
Generally, we are not able to obtain the optimal solution of Problem~\ref{eq-sparse}.
But many approximated solvers have been created to solve this problem.
Matching pursuit (MP), orthogonal matching pursuit (OMP) iteratively add the best basis vector to represent $\mathbf{x}$ which is an approximating solution of problem:

\small{
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{a}} & \|\mathbf{y}-\mathbf{X}\mathbf{a}\|_2^2 \\
\mathrm{s.t.} & \|\mathbf{a}\|_0 \leq s
\end{array}
\end{equation}
}

Here $s$ denotes the number of chosen basis vectors which stands for the sparsity of this problem.
Under certain assumptions, the solution of OMP algorithm converges to the real solution.

Another widely-used technique for solving sparse problem is to use $\ell_1$ norm to replace $\ell_0$ norm in Problem~\ref{eq-sparse}.
The least absolute shrinkage and selection operator (LASSO) approach uses the constraint that $\|\mathbf{a}\|_1$ is no greater than a given value $\epsilon$

\small{
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{a}} & \|\mathbf{y}-\mathbf{X}\mathbf{a}\|_2^2\\
\mathrm{s.t.} & \|\mathbf{a}\|_1 \leq \epsilon
\end{array}
\end{equation}
}

This problem is also equivalent to an unconstrained minimization of:
\small{
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{a}} & \|\mathbf{y}-\mathbf{X}\mathbf{a}\|_2^2 + \beta \|\mathbf{a}\|_1
\end{array}
\end{equation}
}

This problem is solved using general convex optimization methods like quadratic programming, as well as by specific approaches like the least angle regression algorithm.

As discussed above, we show some basic methods of finding the coefficients when the basis vectors are given.
Another big collection of problems discusses algorithms of learning the dictionary or the basis vectors.
One algorithm called K-SVD uses an iterative procedure by iteratively updating the coefficients $\mathbf{a}$ and dictionary $\mathbf{X}$

\small{
\begin{equation}
\begin{array}{cl}
\min_{\mathbf{X},\mathbf{a}} & \|\mathbf{y}-\mathbf{X}\mathbf{a}\| \\
\mathrm{s.t.} & \|\mathbf{a}\|_0 \leq s
\end{array}
\end{equation} 
}